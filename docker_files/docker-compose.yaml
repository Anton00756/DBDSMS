version: "3.9"

services:
  zookeeper:
    image: docker.io/bitnami/zookeeper:3.8
    container_name: zookeeper
    restart: unless-stopped
    ports:
      - "2181"
    volumes:
      - zookeeper_data:/bitnami
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    networks:
      - node-network

  kafka:
    image: docker.io/bitnami/kafka:3.3.1-debian-11-r38
    container_name: kafka
    restart: unless-stopped
    ports:
      - "9092:9092"
    volumes:
      - kafka_data:/bitnami
    environment:
      - KAFKA_CFG_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CLIENT:PLAINTEXT,EXTERNAL:PLAINTEXT
      - KAFKA_CFG_LISTENERS=CLIENT://:9092,EXTERNAL://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=CLIENT://kafka:9092,EXTERNAL://localhost:9093
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=CLIENT
      - KAFKA_LISTENERS=EXTERNAL_SAME_HOST://:29092,INTERNAL://:9092
      - KAFKA_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL_SAME_HOST://localhost:29092
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL_SAME_HOST:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=INTERNAL
    depends_on:
      - zookeeper
    networks:
      - node-network

  init-kafka:
    image: confluentinc/cp-kafka:6.1.1
    container_name: init_kafka
    depends_on:
      - kafka
    networks:
      - node-network
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      kafka-topics --bootstrap-server kafka:9092 --list
      
      echo -e 'Creating kafka topics'
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic calls --replication-factor 3 --partitions 10
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic person_calls --replication-factor 3 --partitions 10
      
      echo -e 'Successfully created the following topics:'
      kafka-topics --bootstrap-server kafka:9092 --list
      "

  greenplum:
    image: ionxwood/greenplum:6.21.0
    container_name: greenplum
    restart: unless-stopped
    ports:
      - "5432:5432"
    networks:
      - node-network
    volumes:
      - greenplum_data:/srv
    environment:
      MALLOC_ARENA_MAX: 1
      TZ: Europe/Moscow
      PGPASSWORD: ${GP_PASSWORD}
    env_file:
      - ${PWD}/.env
    healthcheck:
      #      test: "/usr/local/greenplum-db-6.21.0/bin/pg_isready &&
      #        /usr/local/greenplum-db-6.21.0/bin/psql -h ${GP_HOST} -U ${GP_USER} -d ${GP_DB} -f /init.sql"
      test: "/usr/local/greenplum-db-6.21.0/bin/pg_isready"
      interval: 1s
      timeout: 3s
      retries: 100

  minio:
    image: quay.io/minio/minio:RELEASE.2023-09-23T03-47-50Z
    container_name: minio
    command: server /data --address :9000 --console-address :9001
    restart: unless-stopped
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    env_file:
      - ${PWD}/.env
    networks:
      - node-network

  redis:
    image: redis:7.2.1
    container_name: redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    networks:
      - node-network
    volumes:
      - redis_data:/data

  create_minio_buckets:
    image: minio/mc:RELEASE.2023-09-22T05-07-46Z
    container_name: create_minio_buckets
    depends_on:
      - minio
    networks:
      - node-network
    entrypoint: >
      /bin/sh -c "
      /usr/bin/mc alias set myminio http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD};
      /usr/bin/mc mb myminio/results;
      /usr/bin/mc anonymous set public myminio/results;
      exit 0;
      "

  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: prometheus
    restart: unless-stopped
    volumes:
      - ${PWD}/services/prometheus/config.yml:/etc/prometheus/config.yml
      - prometheus_data:/prometheus
    command: --config.file=/etc/prometheus/config.yml
    ports:
      - "9090:9090"
    environment:
      TZ: "Europe/Moscow"
    networks:
      - node-network

  grafana:
    image: grafana/grafana:10.1.2
    container_name: grafana
    restart: unless-stopped
    depends_on:
      - prometheus
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    environment:
      TZ: "Europe/Moscow"
    networks:
      - node-network

  taskmanager:
    image: tosha/pyflink_image:latest
    depends_on:
      - pyflink
    command: taskmanager
    networks:
      - node-network
    env_file:
      - ${PWD}/services/pyflink/.env
      - ${PWD}/.env
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: pyflink
        taskmanager.numberOfTaskSlots: ${TASK_MANAGER_SLOT_COUNT}
        s3.endpoint: http://minio:9000
        s3.path-style-access: true
        s3.access-key: ${MINIO_ROOT_USER} 
        s3.secret-key: ${MINIO_ROOT_PASSWORD}
        state.backend: filesystem
        s3.connection.maximum: 100
        metrics.reporters: prom
        metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporterFactory
        metrics.reporter.prom.port: 9999

  pyflink:
    image: tosha/pyflink_image:latest
    container_name: pyflink
    networks:
      - node-network
    volumes:
      - ${PWD}/services/orchestrator/config.json:/work_dir/job_config.json
    depends_on:
      init-kafka:
        condition: service_completed_successfully
      greenplum:
        condition: service_healthy
      create_minio_buckets:
        condition: service_started
      redis:
        condition: service_started
      prometheus:
        condition: service_started
      data_generator:
        condition: service_started
    env_file:
      - ${PWD}/services/pyflink/.env
      - ${PWD}/.env
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: pyflink
        s3.endpoint: http://minio:9000
        s3.path-style-access: true
        s3.access-key: ${MINIO_ROOT_USER} 
        s3.secret-key: ${MINIO_ROOT_PASSWORD}
        state.backend: filesystem
        s3.connection.maximum: 100
        metrics.reporters: prom
        metrics.reporter.prom.factory.class: org.apache.flink.metrics.prometheus.PrometheusReporter
        metrics.reporter.prom.port: 9999
  #    entrypoint: [ 'sleep', '10000s' ]

  orchestrator:
    image: tosha/orchestrator_image:latest
    container_name: orchestrator
    restart: unless-stopped
    networks:
      - node-network
    ports:
      - "5000:5000"
    depends_on:
      - pyflink
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ${PWD}/services/orchestrator/config.json:/work_dir/service/config.json
    env_file:
      - ${PWD}/services/orchestrator/.env
      - ${PWD}/.env

  data_generator:
    image: tosha/data_generator_image:latest
    container_name: data_generator
    networks:
      - node-network
    depends_on:
      - init-kafka
      - redis
    env_file:
      - ${PWD}/.env

  result_viewer:
    image: tosha/result_viewer_image:latest
    container_name: result_viewer
    networks:
      - node-network
    depends_on:
      - init-kafka
      - pyflink
    env_file:
      - ${PWD}/services/result_viewer/.env
      - ${PWD}/.env

volumes:
  zookeeper_data:
    driver: local
  kafka_data:
    driver: local
  greenplum_data:
    driver: local
  minio_data:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

networks:
  node-network:
    driver: bridge